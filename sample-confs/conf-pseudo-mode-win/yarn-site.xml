<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

  <!-- RESOURCE MANAGER -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>localhost</value>
    <description>The hostname of the RM.</description>
  </property>    

  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>${yarn.resourcemanager.hostname}:8088</value>
    <description>The http address of the RM web application.</description>
  </property>

  <property>
    <name>yarn.resourcemanager.address</name>
    <value>${yarn.resourcemanager.hostname}:8032</value>
    <description>The address of the applications manager interface in 
    the RM.</description>
  </property>

  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>${yarn.resourcemanager.hostname}:8031</value>
    <description>host is the hostname of the resource manager and 
    port is the port on which the NodeManagers contact the Resource Manager.
    </description>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>${yarn.resourcemanager.hostname}:8030</value>
    <description>host is the hostname of the resourcemanager and port is the port
    on which the Applications in the cluster talk to the Resource Manager.
    </description>
  </property>


  <!-- SCHEDULER -->

  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>256</value>
    <description>The minimum allocation for every container request at the RM,
    in MBs. Memory requests lower than this won't take effect,
    and the specified value will get allocated at minimum.</description>
  </property>

  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>512</value>
    <description>The maximum allocation for every container request at the RM,
    in MBs. Memory requests higher than this won't take effect,
    and will get capped to this value.</description>
  </property>

 <property>
    <name>yarn.scheduler.increment-allocation-mb</name>
    <value>128</value>
    <description>Memory allocations must be a multiple of this value.</description>
</property>

  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
    <description>The minimum allocation for every container request at the RM,
    in terms of virtual CPU cores. Requests lower than this won't take effect,
    and the specified value will get allocated the minimum.</description>
  </property>

  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>2</value>
    <description>The maximum allocation for every container request at the RM,
    in terms of virtual CPU cores. Requests higher than this won't take effect,
    and will get capped to this value.</description>
  </property>

 <property>
    <name>yarn.scheduler.increment-allocation-vcores</name>
    <value>1</value>
    <description>Vcore allocations must be a multiple of this value.</description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>0.5</value>
  </property>


  <!-- NODE MANAGER -->
  <property>
    <name>yarn.nodemanager.hostname</name>
    <value>localhost</value>
    <description>The hostname of the NM.</description>
  </property>

  <property>
    <name>yarn.nodemanager.address</name>
    <value>${yarn.nodemanager.hostname}:38102</value>
    <description>The address of the container manager in the NM.</description>
  </property>

  <property>
    <description>NM Webapp address.</description>
    <name>yarn.nodemanager.webapp.address</name>
    <value>${yarn.nodemanager.hostname}:8042</value>
  </property>

  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/user/hadoop-yarn/nm/applogs</value>
    <description>directory on hdfs where the application logs are moved to</description>
  </property>

  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/Yarn/nm/userlogs</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
    <description>shuffle service that needs to be set for Map Reduce to run </description>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>1024</value>
    <description>Amount of physical memory, in MB, that can be allocated 
    for containers.</description>
  </property>

  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>2</value>
    <description>Number of CPU cores that can be allocated 
    for containers.</description>
  </property>

  <property>
   <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
    <description>Whether virtual memory limits will be enforced for containers</description>
  </property>

  <property>
   <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>5</value>
    <description>Ratio between virtual memory to physical memory when setting memory limits for containers</description>
  </property>

  <property>
     <name>yarn.application.classpath</name>
     <value>
%HADOOP_CONF_DIR%,
%HADOOP_COMMON_HOME%/share/hadoop/common/*,
%HADOOP_COMMON_HOME%/share/hadoop/common/lib/*,
%HADOOP_HDFS_HOME%/share/hadoop/hdfs/*,
%HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/*,
%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*,
%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*,
%HADOOP_YARN_HOME%/share/hadoop/yarn/*,
%HADOOP_YARN_HOME%/share/hadoop/yarn/lib/*
    </value>
    <description>CLASSPATH for YARN applications. A comma-separated list
    of CLASSPATH entries</description>
  </property>

  <property>
    <name>yarn.app.mapreduce.am.resource.cpu-vcores</name>
    <value>1</value>
    <description>AM container vcore reservation.</description>
  </property>

  <property>
    <name>yarn.app.mapreduce.am.resource.mb</name>
    <value>384</value>
    <description>AM container memory reservation.</description>
  </property>

  <property>
    <name>arn.app.mapreduce.am.command-opts</name>
    <value>-Xmx346m</value>
    <description>AM Java heap size.</description>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.hungarian.HungarianScheduler</value>
    <description>The class to use as the resource scheduler.</description>
  </property>

  <property>
    <name>yarn.scheduler.hungarian.enable-tiers</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.app.mapreduce.am.task.assignment.solver.class</name>
    <value>org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AMAssignmentSolverHungarian</value>
  </property>

  <property>
    <name>yarn.app.mapreduce.am.task.assignment.solver.enable.tiers</name>
    <value>true</value>
  </property>
  
</configuration>
